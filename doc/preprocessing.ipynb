{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "There are many preprocessors in scikit-lego and in this document we\n",
    "would like to highlight a few such that you might be inspired to use\n",
    "pipelines a little bit more flexibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators as Transformers\n",
    "\n",
    "\n",
    "Sometimes you'd like the output of a model to be available as a feature\n",
    "that you might use as input for another model. The issue here is that\n",
    "scikit learn pipelines usually only allow a single model at the end of\n",
    "a pipeline. One solution to this problem is to turn the model into a transformer.\n",
    "To convert a model to become a transformer you can use the `EstimatorTransformer`\n",
    "from the `meta` module.\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "Let's demonstrate one example. Below we describe how to create a pipeline\n",
    "with two models that each see the same dataset. Note that the output of this\n",
    "pipeline is still only a transformer pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](_static/estimator-transformer-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "from sklego.meta import EstimatorTransformer\n",
    "from sklego.preprocessing import ColumnSelector\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "X = np.random.uniform(0, 1, (n, 2))\n",
    "y = X.sum(axis=1) + np.random.uniform(0, 1, (n,))\n",
    "df = pd.DataFrame({\"x1\": X[:, 0], \"x2\": X[:, 1], \"y\": y})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"grab_columns\", ColumnSelector([\"x1\", \"x2\"])),\n",
    "    (\"ml_features\", FeatureUnion([\n",
    "        (\"model_1\",  EstimatorTransformer(LinearRegression())),\n",
    "        (\"model_2\",  EstimatorTransformer(Ridge()))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipeline.fit(df, y).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "\n",
    "Here's another example that works a little bit differently. Here\n",
    "we have two models that each see different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](_static/estimator-transformer-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"grab_columns\", ColumnSelector([\"x1\", \"x2\"])),\n",
    "    (\"ml_features\", FeatureUnion([\n",
    "        (\"p1\", Pipeline([\n",
    "            (\"grab1\", ColumnSelector([\"x1\"])),\n",
    "            (\"mod1\", EstimatorTransformer(LinearRegression()))\n",
    "        ])),\n",
    "        (\"p2\", Pipeline([\n",
    "            (\"grab2\", ColumnSelector([\"x2\"])),\n",
    "            (\"mod2\", EstimatorTransformer(LinearRegression()))\n",
    "        ]))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipeline.fit(df, y).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Capping\n",
    "\n",
    "Some models are great at interpolation but less good at extrapolation.\n",
    "One way to potentially circumvent this problem is by capping extreme\n",
    "valeus that occur in the dataset **X**.\n",
    "\n",
    "![img3](_static/column-capper.png)\n",
    "\n",
    "To see how they work we demonstrate a few examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklego.preprocessing import ColumnCapper\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(0, 1, (100000, 2))\n",
    "\n",
    "cc = ColumnCapper()\n",
    "output = cc.fit(X).transform(X)\n",
    "print(f\"min capped at  5th quantile: {output.min(axis=0)}\")\n",
    "print(f\"max capped at 95th quantile: {output.max(axis=0)}\")\n",
    "\n",
    "cc = ColumnCapper(quantile_range=(10, 90))\n",
    "output = cc.fit(X).transform(X)\n",
    "print(f\"min capped at 10th quantile: {output.min(axis=0)}\")\n",
    "print(f\"max capped at 90th quantile: {output.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the column capper does not deal with missing values\n",
    "but it does support pandas dataframes as well as infinite values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[0.0, np.inf],\n",
    "                [-np.inf, 1.0]])\n",
    "cc.transform(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patsy Formulas\n",
    "\n",
    "If you're used to the statistical programming language R you might have\n",
    "seen a formula object before. This is an object that represents a shorthand\n",
    "way to design variables used in a statistical model. The python project [patsy](https://patsy.readthedocs.io/en/latest/)\n",
    "took this idea and made it available for python. From sklego we've made a\n",
    "wrapper such that you can also use these in your pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklego.preprocessing import PatsyTransformer\n",
    "\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, 4, 5],\n",
    "                   \"b\": [\"yes\", \"yes\", \"no\", \"maybe\", \"yes\"],\n",
    "                   \"y\": [2, 2, 4, 4, 6]})\n",
    "X, y = df[[\"a\", \"b\"]], df[[\"y\"]].values\n",
    "pt = PatsyTransformer(\"a + np.log(a) + b\")\n",
    "pt.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the first column contains the constant array\n",
    "equal to one. You might also expect 3 dummy variable columns instead of 2.\n",
    "This is because the design matrix from patsy attempts to keep the\n",
    "columns in the matrix linearly independent of each other.\n",
    "\n",
    "If this is not something you'd want to create you can choose to omit\n",
    "it by indicating \"-1\" in the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PatsyTransformer(\"a + np.log(a) + b - 1\")\n",
    "pt.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that now the constant array is gone and it is replaced with\n",
    "a dummy array. Again this is now possible because patsy wants to guarantee\n",
    "that each column in this matrix is linearly independent of each other.\n",
    "\n",
    "The formula syntax is pretty powerful, if you'd like to learn we refer you\n",
    "to [formulas](https://patsy.readthedocs.io/en/latest/formulas.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating Basis Function Transformer\n",
    "\n",
    "Some variables are of a circular nature. For example, the days of the year, 1-Jan-2019 (day 1) is just as close to 2-Jan-2019 (day 2) as it is to 31-Dec-2018 (day 365). If you would encode day of year numerically you would lose this information, as 1 close 2 to but far from 365. The repeating basis function transformer can remedy this problem.\n",
    "\n",
    "The transformer selects a column and transforms it with a given number of repeating (radial) basis functions, which have a bell curve shape. The basis functions are equally spaced over the input range. The key feature of repeating basis functions is that they are continuous when moving\n",
    "from the max to the min of the input range. As a result these repeating basis functions can capture how close each datapoint is to the center of each repeating basis function, even when the input data has a circular nature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let's make some random data to start with. We have input data `day`, `day_of_year` and target `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate features\n",
    "X = pd.DataFrame()\n",
    "X['day'] = np.arange(4*365)\n",
    "X['day_of_year'] = 1 + X['day'] % 365\n",
    "\n",
    "# generate target\n",
    "signal1 = 4 + 3*np.sin(X['day']/365*2*np.pi) \n",
    "signal2 = 4 * np.sin(X['day']/365*4*np.pi+365/2)\n",
    "noise = np.random.normal(0, 0.9, len(X['day']))       \n",
    "y = signal1 + signal2 + noise\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(17,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(X['day'],y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create repeating basis functions based on `day_of_year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklego.preprocessing import RepeatingBasisFunction\n",
    "\n",
    "N_PERIODS = 5\n",
    "rbf = RepeatingBasisFunction(n_periods=N_PERIODS,\n",
    "                             remainder='passthrough',\n",
    "                             column='day_of_year',\n",
    "                             input_range=(1,365))\n",
    "rbf.fit(X)\n",
    "Xt = rbf.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=Xt.shape[1], figsize=(17,12))\n",
    "for i in range(Xt.shape[1]):\n",
    "    axes[i].plot(X['day'], Xt[:,i]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `day_of_year` feature has been replaced with `N_PERIODS` repeating basis functions. These are bell curves that are equidistant over the 1-365 range. Each curve captures the information of \"being close to\" a particular `day_of_year`. For example, the curve in the top row captures how close a day is to new year's day. It peaks on day 1 with a value of 1 and smoothly drops at an equal rate in December and in the rest of January. \n",
    "\n",
    "Note, how the `day` feature still exists, in the transformed feature set as a result of the `remainder='passthrough'` setting. The default setting `remainder='drop'` will only keep the repeating basis functions and drop all columns of the original dataset.\n",
    "\n",
    "## Example \n",
    "\n",
    "Here's an example below where we use this preprocessing step to generate features for a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('rbf', rbf),\n",
    "    ('mod', LinearRegression())\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(17,3))\n",
    "plt.plot(X['day'], y)\n",
    "plt.plot(X['day'], pipe.fit(X, y).predict(X), linewidth=3.0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
