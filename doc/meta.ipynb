{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Models\n",
    "\n",
    "Certain models in scikit-lego are \"meta\". Meta models are\n",
    "models that depend on other estimators that go in and these\n",
    "models will add features to the input model. One way of thinking\n",
    "of a meta model is to consider it to be a way to \"decorate\" a\n",
    "model.\n",
    "\n",
    "This part of the documentation will highlight a few of them.\n",
    "\n",
    "## Thresholder\n",
    "\n",
    "The thresholder can help tweak recall and precision of a model\n",
    "by moving the threshold value of `predict_proba`. Commonly this\n",
    "threshold is set at 0.5 for two classes. This meta-model can\n",
    "decorate an estimator with two classes such that the threshold\n",
    "moves.\n",
    "\n",
    "We demonstrate the working below. First we'll generate a skewed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, make_scorer\n",
    "\n",
    "from sklego.meta import Thresholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(1000, centers=[(0, 0), (1.5, 1.5)], cluster_std=[1, 0.5])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll make a cross validation pipeline to try out this thresholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"model\", Thresholder(LogisticRegression(solver='lbfgs'), threshold=0.1))\n",
    "])\n",
    "\n",
    "mod = GridSearchCV(estimator=pipe,\n",
    "                   param_grid = {\"model__threshold\": np.linspace(0.1, 0.9, 50)},\n",
    "                   scoring={\"precision\": make_scorer(precision_score),\n",
    "                            \"recall\": make_scorer(recall_score),\n",
    "                            \"accuracy\": make_scorer(accuracy_score)},\n",
    "                   refit=\"precision\",\n",
    "                   cv=5)\n",
    "\n",
    "mod.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this cross validation trained, we'll make a chart to show the\n",
    "effect of changing the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(mod.cv_results_)\n",
    " .set_index(\"param_model__threshold\")\n",
    " [['mean_test_precision', 'mean_test_recall', 'mean_test_accuracy']]\n",
    " .plot(figsize=(16, 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the threshold will increase the precision but as expected this is at the\n",
    "cost of recall (and accuracy).\n",
    "\n",
    "## Grouped Estimation\n",
    "\n",
    "<img src=\"_static/grouped-model.png\" width=\"50%\" alt=\"img1\">\n",
    "\n",
    "To help explain what it can do we'll consider three methods to predict\n",
    "the chicken weight. The chicken data has 578 rows and 4 columns\n",
    "from an experiment on the effect of diet on early growth of chicks.\n",
    "The body weights of the chicks were measured at birth and every second\n",
    "day thereafter until day 20. They were also measured on day 21.\n",
    "There were four groups on chicks on different protein diets.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Let's first load a bunch of things to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklego.datasets import load_chicken\n",
    "from sklego.preprocessing import ColumnSelector\n",
    "\n",
    "df = load_chicken(give_pandas=True)\n",
    "\n",
    "def plot_model(model):\n",
    "    df = load_chicken(give_pandas=True)\n",
    "    model.fit(df[['diet', 'time']], df['weight'])\n",
    "    metric_df = df[['diet', 'time', 'weight']].assign(pred=lambda d: model.predict(d[['diet', 'time']]))\n",
    "    metric = mean_absolute_error(metric_df['weight'], metric_df['pred'])\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.scatter(df['time'], df['weight'])\n",
    "    for i in [1, 2, 3, 4]:\n",
    "        pltr = metric_df[['time', 'diet', 'pred']].drop_duplicates().loc[lambda d: d['diet'] == i]\n",
    "        plt.plot(pltr['time'], pltr['pred'], color='.rbgy'[i])\n",
    "    plt.title(f\"linear model per group, MAE: {np.round(metric, 2)}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will be used to explain the steps below.\n",
    "\n",
    "### Model 1: Linear Regression with Dummies\n",
    "\n",
    "First we start with a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = Pipeline([\n",
    "    (\"datagrab\", FeatureUnion([\n",
    "         (\"discrete\", Pipeline([\n",
    "             (\"grab\", ColumnSelector(\"diet\")),\n",
    "             (\"encode\", OneHotEncoder(categories=\"auto\", sparse=False))\n",
    "         ])),\n",
    "         (\"continuous\", Pipeline([\n",
    "             (\"grab\", ColumnSelector(\"time\")),\n",
    "             (\"standardize\", StandardScaler())\n",
    "         ]))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"transform\", feature_pipeline),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "plot_model(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model is linear the dummy variable causes the intercept\n",
    "to change but leaves the gradient untouched. This might not be what\n",
    "we want from a model. So let's see how the grouped model can address\n",
    "this.\n",
    "\n",
    "### Model 2: Linear Regression in GroupedEstimation\n",
    "\n",
    "The goal of the grouped estimator is to allow us to split up our data.\n",
    "The image below demonstrates what will happen.\n",
    "\n",
    "<img src=\"_static/grouped-df.png\" width=\"70%\" alt=\"img2\">\n",
    "\n",
    "We train 5 models in total because the model will also train a\n",
    "fallback automatically (you can turn this off via `use_fallback=False`).\n",
    "The idea behind the fallback is that we can predict something if\n",
    "the group does not appear in the prediction.\n",
    "\n",
    "Each model will accept features that are in `X` that are not\n",
    "part of the grouping variables. In this case each group will\n",
    "model based on the `time` since `weight` is what we're trying\n",
    "to predict.\n",
    "\n",
    "Applying this model to the dataframe is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklego.meta import GroupedEstimator\n",
    "mod = GroupedEstimator(LinearRegression(), groups=[\"diet\"])\n",
    "plot_model(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model looks a bit better.\n",
    "\n",
    "### Model 3: Dummy Regression in GroupedEstimation\n",
    "\n",
    "We could go a step further and train a [DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) per diet\n",
    "per timestep. The code below works similar as the previous example\n",
    "but one difference is that the grouped model does not receive a\n",
    "dataframe but a numpy array.\n",
    "\n",
    "<img src=\"_static/grouped-np.png\" width=\"70%\" alt=\"img3\">\n",
    "\n",
    "Note that we're also grouping over more than one column here.\n",
    "The code that does this is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    (\"datagrab\", FeatureUnion([\n",
    "         (\"discrete\", Pipeline([\n",
    "             (\"grab\", ColumnSelector(\"diet\")),\n",
    "         ])),\n",
    "         (\"continuous\", Pipeline([\n",
    "             (\"grab\", ColumnSelector(\"time\")),\n",
    "         ]))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"transform\", feature_pipeline),\n",
    "    (\"model\", GroupedEstimator(DummyRegressor(strategy=\"mean\"), groups=[0, 1]))\n",
    "])\n",
    "\n",
    "plot_model(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these predictions seems to yield the lowest error but take it\n",
    "with a grain of salt since these errors are only based on the train set.\n",
    "\n",
    "## Decayed Estimation\n",
    "\n",
    "Often you are interested in predicting the future. You use the data from\n",
    "the past in an attempt to achieve this and it could be said that perhaps\n",
    "data from the far history is less relevant than data from the recent past.\n",
    "\n",
    "This is the idea behind the `DecayEstimator` meta-model. It looks at the\n",
    "order of data going in and it will assign a higher importance to recent rows\n",
    "that occurred recently and a lower importance to older rows. Recency is based\n",
    "on the order so it is imporant that the dataset that you pass in is correctly\n",
    "ordered beforehand.\n",
    "\n",
    "We'll demonstrate how it works by applying it on a simulated timeseries problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklego.meta import GroupedEstimator, DecayEstimator\n",
    "from sklego.datasets import make_simpleseries\n",
    "\n",
    "yt = make_simpleseries(seed=1)\n",
    "df = (pd.DataFrame({\"yt\": yt,\n",
    "                   \"date\": pd.date_range(\"2000-01-01\", periods=len(yt))})\n",
    "      .assign(m=lambda d: d.date.dt.month)\n",
    "      .reset_index())\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(make_simpleseries(seed=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two models on this dataset. One model calculates the average\n",
    "value per month in our timeseries and the other does the same thing but will\n",
    "decay the importance of making accurate predictions for the far history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = (GroupedEstimator(DummyRegressor(), groups=[\"m\"])\n",
    "        .fit(df[['m']], df['yt']))\n",
    "\n",
    "\n",
    "mod2 = (GroupedEstimator(DecayEstimator(DummyRegressor(), decay=0.9), groups=[\"m\"])\n",
    "        .fit(df[['index', 'm']], df['yt']))\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(df['yt'], alpha=0.5);\n",
    "plt.plot(mod1.predict(df[['m']]), label=\"grouped\")\n",
    "plt.plot(mod2.predict(df[['index', 'm']]), label=\"decayed\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decay parameter has a lot of influence on the effect of the model but one\n",
    "can clearly see that we shift focus to the more recent data.\n",
    "\n",
    "# Confusion Balancer \n",
    "\n",
    "**Disclaimer**: This is an experimental feature. \n",
    "\n",
    "We added an experimental feature to the meta estimators that can be used to force balance in the confusion matrix of an estimator. The approach works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2, n3 = 100, 500, 50\n",
    "np.random.seed(42)\n",
    "X = np.concatenate([np.random.normal(0, 1, (n1, 2)), \n",
    "                    np.random.normal(2, 1, (n2, 2)),\n",
    "                    np.random.normal(3, 1, (n3, 2))], \n",
    "                   axis=0)\n",
    "y = np.concatenate([np.zeros((n1, 1)), \n",
    "                    np.ones((n2, 1)),\n",
    "                    np.zeros((n3, 1))], \n",
    "                   axis=0).reshape(-1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take this dataset and train a simple classifier against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
    "cfm = confusion_matrix(y, mod.fit(X, y).predict(X))\n",
    "cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is not ideal. This is in part because the dataset is slightly imbalanced but in general it is also because of the way the algorithm works. Let's see if we can learn something else from this confusion matrix. I might transform the counts into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm.T / cfm.T.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the number 0.2346 in the lower left corner. This number represents the probability that the actually class 0 while the model predicts class 1. In math we might write this as $P(C_1 | M_1)$ where $C_i$ denotes the actual label while $M_i$ denotes the label given by the algorithm. \n",
    "\n",
    "The idea now is that we might rebalance our original predictions $P(M_i)$ by multiplying them;\n",
    "\n",
    "$$ P_{\\text{corrected}}(C_1) = P(C_1|M_0) p(M_0) + P(C_1|M_1) p(M_1) $$ \n",
    "\n",
    "In general this can be written as; \n",
    "\n",
    "$$ P_{\\text{corrected}}(C_i) = \\sum_j P(C_i|M_j) p(M_j) $$\n",
    "\n",
    "In laymens terms; we might be able to use the confusion matrix to learn from our mistakes. By how much we correct is something that we can tune with a hyperparameter. \n",
    "\n",
    "$$ P_{\\text{corrected}}(C_i) = \\alpha \\sum_j P(C_i|M_j) p(M_j) + (1-\\alpha) p(M_j) $$\n",
    "\n",
    "We'll perform an optimistic demonstration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positives(mod, x, y):\n",
    "    return (mod.predict(x) != y)[y == 1].sum()\n",
    "\n",
    "def false_negatives(mod, x, y):\n",
    "    return (mod.predict(x) != y)[y == 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklego.meta import ConfusionBalancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mod = ConfusionBalancer(LogisticRegression(solver='lbfgs', max_iter=1000), alpha=1.0)\n",
    "\n",
    "grid = GridSearchCV(cf_mod, \n",
    "                    param_grid={'alpha': np.linspace(-1.0, 3.0, 31)},\n",
    "                    scoring={\n",
    "                        \"accuracy\": make_scorer(accuracy_score),\n",
    "                        \"positives\": false_positives,\n",
    "                        \"negatives\": false_negatives\n",
    "                    },\n",
    "                    n_jobs=-1,\n",
    "                    iid=True,\n",
    "                    return_train_score=True,\n",
    "                    refit=\"negatives\",\n",
    "                    cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid.fit(X, y).cv_results_)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(121)\n",
    "plt.plot(df['param_alpha'], df['mean_test_positives'], label=\"false positives\")\n",
    "plt.plot(df['param_alpha'], df['mean_test_negatives'], label=\"false negatives\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(df['param_alpha'], df['mean_test_accuracy'], label=\"test accurracy\")\n",
    "plt.plot(df['param_alpha'], df['mean_train_accuracy'], label=\"train accurracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we can pick a value for $\\alpha$ such that the confusion matrix is balanced. There's also a modest increase in accuracy for this balancing moment. \n",
    "\n",
    "It should be emphesized though that this feature is **experimental**. There have been dataset/model combinations where this effect seems to work very well while there have also been situations where this trick does not work at all. It also deserves mentioning that there might be alternative to your problem. If your dataset is suffering from a huge class imbalance then you might be better off by having a look at the [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/) project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
